{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "moyz5L8Gqhd1",
        "outputId": "6ba56fed-60b3-4e6f-e692-bfc1482aa0af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load Saved Model From Google Drive\n",
            "iteration: 0 total loss tensor(17.6385, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 150 total loss tensor(17.6383, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 300 total loss tensor(17.6382, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 450 total loss tensor(17.6380, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 600 total loss tensor(17.6378, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 750 total loss tensor(17.6379, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 900 total loss tensor(17.6378, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 1050 total loss tensor(17.6377, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 1200 total loss tensor(17.6377, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 1350 total loss tensor(17.6376, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n",
            "iteration: 1500 total loss tensor(17.6375, grad_fn=<AddBackward0>) avg loss tensor(0.0923, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from numpy.core.fromnumeric import argsort, mean, squeeze\n",
        "from torch import tensor\n",
        "from torch.functional import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math as m\n",
        "import time\n",
        "import os \n",
        "import random\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CPU\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NLR3(nn.Module):\n",
        "    def __init__(self,netin,netout,nethidden):\n",
        "      super().__init__()\n",
        "      self.netmodel= torch.nn.Sequential(torch.nn.Linear(netin, nethidden),torch.nn.Tanh(),torch.nn.Linear(nethidden, netout))\n",
        "    def myforward (self,inv):\n",
        "      outv=self.netmodel(inv)\n",
        "      return outv\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/Master/Files/FeaturesToFiles172/Features172QueryStructureallF.txt', 'rb') as fp:\n",
        "       AllData=pickle.load( fp)\n",
        "\n",
        "\n",
        "\n",
        "target=[d['Target152F'] for d in AllData]\n",
        "inp=[d['TargetCaptionF'] for d in AllData]\n",
        "target=torch.tensor(target).to(device)\n",
        "inp=torch.tensor(inp).to(device)\n",
        "\n",
        "hidden=2500\n",
        "l_r=0.2\n",
        "epoch=100000\n",
        "batch_size=900\n",
        "save_duration=150\n",
        "seed=100\n",
        "min_error=0.1\n",
        "\n",
        "model_mlp=NLR3(inp.shape[1],target.shape[1],hidden).to(device)\n",
        "if(os.path.isfile('/content/drive/My Drive/Master/Files/SavedModels/UltraNetB152_2500.pth')):\n",
        "  print('Load Saved Model From Google Drive')\n",
        "  model_mlp.load_state_dict(torch.load( '/content/drive/My Drive/Master/Files/SavedModels/UltraNetB152_2500.pth', map_location=torch.device('cpu') ))\n",
        "\n",
        "loss_fn2=torch.nn.CosineSimilarity()\n",
        "loss_fn=torch.nn.MSELoss() \n",
        "optimizer=torch.optim.SGD(model_mlp.parameters(), lr=l_r)\n",
        "s=0\n",
        "sweep_range=inp.shape[0]%batch_size\n",
        "\n",
        "\n",
        "optimizer.zero_grad()\n",
        "\n",
        "if(os.path.isfile('/content/drive/My Drive/Master/Files/SavedModels/UltraNetBlosses1522.pkl')):\n",
        "  with open('/content/drive/My Drive/Master/Files/SavedModels/UltraNetBlosses1522.pkl', 'rb') as fp:\n",
        "       totallosses=pickle.load( fp)\n",
        "       #print(totallosses)\n",
        "else:\n",
        "  totallosses=[]\n",
        "\n",
        "for j in range(epoch):\n",
        "  total_loss=0\n",
        "  \n",
        "  for l in range(int(inp.shape[0]/batch_size)):\n",
        "    item_batch = inp[l*batch_size+s:(l+1)*batch_size+s,:].to(device)\n",
        "    target_batch=target[l*batch_size+s:(l+1)*batch_size+s,:].to(device)\n",
        "    netoutbatch=model_mlp.myforward(item_batch).to(device)\n",
        "    \n",
        "    loss = torch.mean(torch.abs(loss_fn2(target_batch,netoutbatch)))\n",
        "    loss=1-loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss+=loss\n",
        "  \n",
        "  if (total_loss<min_error):\n",
        "    break\n",
        "  \n",
        "  if(j%save_duration==0):\n",
        "    print('iteration:',j, 'total loss',total_loss,'avg loss', total_loss/(inp.shape[0]/batch_size))\n",
        "    torch.save(model_mlp.state_dict(), '/content/drive/My Drive/Master/Files/SavedModels/UltraNetB152_2500.pth')\n",
        "    totallosses.append(total_loss)\n",
        "    with open('/content/drive/My Drive/Master/Files/SavedModels/UltraNetBlosses1522.pkl', 'wb') as fp:\n",
        "      pickle.dump( totallosses, fp)\n",
        "    total_loss=0\n",
        "    #print('model & Loss  saved ')\n",
        "  \n",
        "  \n",
        "  \n",
        "  s+=1\n",
        "  if s==sweep_range:\n",
        "      s=0\n",
        "    \n",
        "\n",
        "print('Finished Training')\n",
        "torch.save(model_mlp.state_dict(), '/content/drive/My Drive/Master/Files/SavedModels/UltraFinal_NetB152_2500.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "UltraNetB152_2500.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}